RL EXPLAINED SECTION
-----------------------------------------------------------

RL is a class of algorithms in Machine learning which can learn to navigate an environment in such a way to maximize the cumulative reward it receives. By initially making random actions in states, and being informed of the quality of the action chosen, the agent is able to iteratively approach more optimal generalizations about its environment.

A valuable aspect of RL is that we do not even need to understand what the optimal solution will be which maximizes the reward. All that is needed for learning to take place is an environment, a way to observe that environment, and a reward signal which determines the value of any state given the observation representation of the state. For this reason, one particularly important and challenging aspect of RL is how to model the environment states, and what reward we should assign to those states.

Why do we even need this RL stuff? Can't we just pre-compute the most optimal action to take for any given state? The problem is the scale of possible states to experience and actions to choose from. As an example, if we take an atari screen's pixels as input to learn to play some game, where the size of the screen is 160x192 pixels in grayscale, each pixel would be able to take on 256 different values. And there being 160x192 different pixels, the total number of states this policy function could receive as input would be 256^(160x192). (costa huang dissertation) For context there are 10^80 atoms in the known universe so this problem is computationally infeasible to pre-compute a state to action mapping table.

The essence of RL is to learn some function approximation called the policy which takes in as input the state of the agent in the environment, and outputs an action to take to move us from the current state to a new state. The chain of state action action pairs an agent experiences is called a trajectory. This trajectory forms an episode, and can either end by reaching a terminal state (out of bounds, or final reward achieved, etc.) or by reaching a trajectory max length T. 

Fundamentally there are only two phases in training. The first is the data collection step, which is when the agents navigate the environment, collecting information like the state transitions and actions taken, the probability outputted for the action chosen, the reward received from the new state, and more. The next step is the optimization step where we take this information and optimize our policy function so that we discourage taking actions in states which received low reward, and encourage taking actions in states where we received large reward.


MARKOV DECISION PROCESS (MDP)
-----------------------------------------------------------

We now must answer the question of how an agent perceives the environment, and how to make decisions based on this perception. A process is said to satisfy the Markov property if decisions are based solely on the current state at time t. This means that all relevant information an agent needs to make educated future predictions must be determined by nothing but knowledge of the current state, so there is no need for any sort of memory. (PPO explanation paper)

\frac{}{}